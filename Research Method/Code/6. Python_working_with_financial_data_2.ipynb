{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Working with financial data\n",
    "\n",
    "Before importing the data, first some package imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's try a built-in Python function to import a CSV file (https://docs.python.org/3/library/functions.html#open) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\r.skripnik\\\\Desktop\\\\Python-R\\\\price_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-d8b10b0d75be>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Enter the path where you saved the price_data.csv file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mr'C:\\Users\\r.skripnik\\Desktop\\Python-R\\price_data.csv'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\r.skripnik\\\\Desktop\\\\Python-R\\\\price_data.csv'"
     ]
    }
   ],
   "source": [
    "#Enter the path where you saved the price_data.csv file\n",
    "filename = r'C:\\Users\\r.skripnik\\Desktop\\Python-R\\price_data.csv'\n",
    "f = open(filename, 'r')\n",
    "f.readlines()[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not convenient working with this data. Fortunately, **pandas** provides a number of different functions and **DataFrame** methods to import data stored in different formats (CSV, SQL, Excel, etc.) and to export data to different formats. The following code uses the **pd.read_csv()** function to import the time series data set from the CSV file (https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(filename,\n",
    "                   index_col=0,\n",
    "                   parse_dates=True)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data used is from YAHOO! finance. Here:\n",
    "\n",
    "* AAPL - Apple stock;\n",
    "* AMZN - Amazon stock;\n",
    "* GDX - VanEck Vectors Gold Miners ETF;\n",
    "* MSFT - Microsoft stock;\n",
    "* SPY - SPDR S&P 500 ETF;\n",
    "* VIX - Volatility index.\n",
    "\n",
    "The **DataFrame** contains daily adjusted closing prices for the above instruments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.plot(figsize=(10, 12),\n",
    "          subplots=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A next step the financial analyst might take is to have a look at different summary statistics for the data (https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.describe.html) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also options, of course, to customize what type of statistic to derive and display. We can do it with simple methods, such as **.mean()** (and others) or choose desired statistics and display them all at once, using **.aggregate()** (https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.aggregate.html) method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.aggregate([np.min,\n",
    "                np.mean,\n",
    "                np.std,\n",
    "                np.median,\n",
    "                np.max]).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most statistical analysis methods are based on changes of a time series over time and not the absolute values themselves. There are multiple options to calculate the changes of a time series over time, such as absolute differences, percentage changes and logarithmic returns. First, the absolute differences for which **pandas** provides a special **.diff()** (https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.diff.html) method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.diff().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.diff().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a statistics point of view, absolute changes are not optimal because they are dependent on the scale of the time series data itself. Therefore, percentage changes are usually preferred. The following code derives the percentage changes using **.pct_change()** (https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.pct_change.html) method and visualizes their mean values per column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.pct_change().round(4).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.pct_change().mean().plot(kind='bar',\n",
    "                              figsize=(10, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an alternative to percentage returns, log returns can be used. In some scenarios, they are more easy to handle and therefore often preferred in a financial context. To calculate log returns, we need to first lag the data (shift prices one period back) using **.shift()** method (https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.shift.html) ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shift(1).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rets = np.log(data / data.shift(1))\n",
    "rets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have calculated log values, we need to convert them back using exponent function. The cumulative log returns for the financial times series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rets.cumsum().apply(np.exp).plot(figsize=(10, 6), grid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resampling is an important operation on financial time series data. Usually, this takes on the form of up-sampling, meaning that, for example, a time series with daily observations is resampled to a time series with weekly or monthly observations. In **pandas** it is done using **.resample()** method (https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.resample.html). Let's see how to resample with weekly and monthly frequencies, and how to plot monthly data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.resample('1w').last().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.resample('1m').last().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rets.cumsum().apply(np.exp).resample('1m').last().plot(figsize=(10, 6), grid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is financial tradition to work with rolling statistics, often also called financial indicators or financial studies. Such rolling statistics are basic tools for financial chartists and technical traders. This section uses **.rolling()** (https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.rolling.html) method and works with a single financial time series only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sym = 'GDX'\n",
    "new_data = pd.DataFrame(data[sym]).dropna()\n",
    "new_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "window = 20\n",
    "new_data['Min'] = new_data[sym].rolling(window=window).min()\n",
    "\n",
    "new_data['Mean'] = new_data[sym].rolling(window=window).mean()\n",
    "\n",
    "new_data['Std'] = new_data[sym].rolling(window=window).std()\n",
    "\n",
    "new_data['Median'] = new_data[sym].rolling(window=window).median()\n",
    "\n",
    "new_data['Max'] = new_data[sym].rolling(window=window).max()\n",
    "\n",
    "new_data['Ewma'] = new_data[sym].ewm(halflife=0.5,\n",
    "                                     min_periods=window).mean()\n",
    "new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ax = new_data[['Min', 'Mean', 'Max']].iloc[-200:].plot(figsize=(10, 6),\n",
    "                                                       style=['g--', 'r--', 'g--'],\n",
    "                                                       lw=0.8)\n",
    "new_data[sym].iloc[-200:].plot(ax=ax,\n",
    "                               lw=2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rolling statistics are a major tool in the technical analysis of stocks as compared to the fundamental analysis which focuses on financial reports and the strategic positions of the company. A decades-old trading strategy based on technical analysis uses two simple moving averages (SMAs). The idea is that the trader should be long a stock when the shorter-term SMA is above the longer-term SMA and should be short the stock when the opposite holds true. The concepts can be made precise with **pandas** and the capabilities of the **DataFrame** object. Rolling statistics are generally only calculated when there is enough data given the **window** parameter specification. The SMA time series only start at the day for which there is enough data given the specific parametrization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data['SMA1'] = new_data[sym].rolling(window=42).mean()\n",
    "new_data['SMA2'] = new_data[sym].rolling(window=252).mean()\n",
    "new_data[[sym, 'SMA1', 'SMA2']].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data[[sym, 'SMA1', 'SMA2']].plot(figsize=(10, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this context, the SMAs are only a means to an end. They are used to derive positionings to implement a trading strategy. In the following, long position is visualized by a value of 1 and a short position by a value of -1. The change in the position is triggered (visually) by a crossover of the two lines representing the SMA time series (https://numpy.org/doc/stable/reference/generated/numpy.where.html) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data.dropna(inplace=True)\n",
    "new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data['Positions'] = np.where(new_data['SMA1'] > new_data['SMA2'],\n",
    "                                 1,\n",
    "                                 -1)\n",
    "new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = new_data[[sym, 'SMA1', 'SMA2', 'Positions']].plot(figsize=(10, 6),\n",
    "                                                       secondary_y='Positions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trading strategy implicitly derived here only leads to a few trades: only when the position value changes (i.e. a crossover happens), a trade takes place. Including opening and closing trades, this would add up to six trades only in total.\n",
    "\n",
    "The following data set consists of two financial times series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = pd.read_csv(filename,\n",
    "#                  index_col=0,\n",
    "#                  parse_dates=True)\n",
    "\n",
    "data = data[['SPY', '^VIX']].dropna()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.plot(figsize=(10, 6),\n",
    "          subplots=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When plotting (parts of) the two time series in a single plot and with adjusted scalings, the stylized fact of negative correlation between the two indices becomes already evident through simple visual inspection (https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.plot.html) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc['2018-01-01':].plot(figsize=(10, 6),\n",
    "                             secondary_y='^VIX')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As pointed out above, statistical analysis in general relies on returns instead of absolute changes or even absolute values. Therefore, the calculation of log returns first before any further analysis takes place. The following shows the high variability of the log returns over time. For both indices so-called volatility clusters can be spotted. And in general, periods of high volatility in the stock index are accompanied by the same phenomenon in the volatility index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rets = np.log(data / data.shift(1))\n",
    "rets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rets.dropna(inplace=True)\n",
    "rets.plot(figsize=(10, 6),\n",
    "          subplots=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In such a context, the **pandas** **.plotting.scatter_matrix()** (https://pandas.pydata.org/docs/reference/api/pandas.plotting.scatter_matrix.html) plotting function comes in handy for visualizations. It plots the log returns of the two series against each other:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(rets,\n",
    "                           figsize=(10, 6),\n",
    "                           diagonal='hist',\n",
    "                           hist_kwds={'bins': 35})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all these preparations, an ordinary least-squares (OLS) regression analysis can be implemented. **numpy.polyfit()** (https://numpy.org/doc/stable/reference/generated/numpy.polyfit.html) function is used to estimate the coefficients (intercept and the slope coefficient), and **numpy.polyval()** (https://numpy.org/doc/stable/reference/generated/numpy.polyval.html) function is used to predict the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs = np.polyfit(rets['SPY'],\n",
    "                    rets['^VIX'],\n",
    "                    deg=1)\n",
    "coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.polyval(coeffs,\n",
    "                  rets['SPY'])\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following shows a scatter plot of the log returns and the linear regression line through the cloud of dots. The slope is obviously negative providing support for the stylized fact about the negative correlation between the two indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = rets.plot(figsize=(10, 6),\n",
    "               kind='scatter',\n",
    "               x='SPY',\n",
    "               y='^VIX')\n",
    "\n",
    "ax.plot(rets['SPY'],\n",
    "        pred,\n",
    "        'r',\n",
    "        lw=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, consider correlation measures directly. Two such measures are considered: a static one taking into account the complete data set and a rolling one showing the correlation for a fixed window over time. The following illustrates that the correlation indeed varies over time, but it is always negative. I use **.axhline()** (https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.axhline.html) method to plot a horizontal line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rets.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ax = rets['SPY'].rolling(window=252).corr(rets['^VIX']).dropna().plot(figsize=(10, 6))\n",
    "\n",
    "ax.axhline(rets.corr().iloc[0, 1],\n",
    "           c='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This provides strong support for the stylized fact that the S&P 500 and the VIX indices are strongly negatively correlated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the major strengths of **pandas** is that it can read and write different data formats natively, among others, including:\n",
    "\n",
    "* CSV (comma-separated value);\n",
    "* SQL (Structured Query Language);\n",
    "* XLS/XSLX (Microsoft Excel files);\n",
    "* JSON (JavaScript Object Notation);\n",
    "* HTML (HyperText Markup Language).\n",
    "\n",
    "Let's export some data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(6)\n",
    "data = np.random.standard_normal((10000, 5)).round(4)\n",
    "data = pd.DataFrame(data,\n",
    "                    columns=['Col1', 'Col2', 'Col3', 'Col4', 'Col5'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most widely used formats to exchange financial data is the **CSV** format. Although it is not really standardized, it can be processed by any platform and the vast majority of applications. **pandas** makes this whole procedure convenient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = r'C:\\Users\\r.skripnik\\Desktop\\Python-R\\\\'\n",
    "data.to_csv(directory + 'sample.csv')\n",
    "\n",
    "df = pd.read_csv(directory + 'sample.csv',\n",
    "                 index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Col1', 'Col2', 'Col3', 'Col4']].hist(figsize=(10, 6),\n",
    "                                          bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code briefly demonstrates how **pandas** can write data in **Excel** format and read data from **Excel** spreadsheets. Here only 5,000 first rows are used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[:5000].to_excel(directory + 'sample.xlsx')\n",
    "\n",
    "df = pd.read_excel(directory + 'sample.xlsx',\n",
    "                   'Sheet1',\n",
    "                   index_col=0)\n",
    "\n",
    "df.cumsum().plot(figsize=(10, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating the **Excel** spreadsheet file with a smaller subset of the data takes quite a while. This illustrates what kind of overhead the spreadsheet structure brings along with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Exercises._**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 1. Assume you invested 1000 USD in each of MSFT, AMZN stocks and GDX for the entire period (2016-2019). Plot the value of the portfolio over time (daily), together with the value of SPY (assuming you invested 4000 USD). Indicate in the plot when the value of your portfolio was above the value of SPY investment and when it was below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "\n",
    "data = yf.download('AMZN MSFT SPY GDX', start = '2016-01-01', end = '2019-12-31', period = '1d')\n",
    "prices = data['Adj Close']\n",
    "ratios = prices / prices.iloc[0]\n",
    "ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "\n",
    "data = yf.download('AMZN MSFT SPY GDX', start = '2016-01-01', end = '2019-12-31', period = '1d')\n",
    "prices = data['Adj Close']\n",
    "ratios = prices / prices.iloc[0]\n",
    "\n",
    "ratios['Portfolio'] = (ratios['AMZN'] + ratios['GDX'] + ratios['MSFT']) * 1000\n",
    "ratios['SPY_port'] = ratios['SPY'] * 4000\n",
    "\n",
    "ratios['Positions'] = np.where(ratios['Portfolio'] > ratios['SPY_port'],\n",
    "                               1,\n",
    "                               -1)\n",
    "\n",
    "ratios[['Portfolio', 'SPY_port', 'Positions']].plot(figsize=(10, 6),\n",
    "                                                    secondary_y='Positions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 2. In one chart plot two sets of scatterplots: one for SPY returns against VIX returns and one for SPY returns against AAPL returns (resample weekly). Fit a line through each of the cloud of dots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "\n",
    "data = yf.download('AAPL SPY ^VIX', start = '2016-01-01', end = '2019-12-31', period = '1d')\n",
    "prices = data['Adj Close']\n",
    "prices = prices.resample('1w').last()\n",
    "rets = np.log(prices / prices.shift(1))\n",
    "rets.dropna(inplace=True)\n",
    "\n",
    "coeffs_1 = np.polyfit(rets['SPY'],\n",
    "                      rets['^VIX'],\n",
    "                      deg=1)\n",
    "pred_1 = np.polyval(coeffs_1,\n",
    "                    rets['SPY'])\n",
    "\n",
    "coeffs_2 = np.polyfit(rets['SPY'],\n",
    "                      rets['AAPL'],\n",
    "                      deg=1)\n",
    "pred_2 = np.polyval(coeffs_2,\n",
    "                    rets['SPY'])\n",
    "\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "plt.scatter(rets['SPY'], rets['^VIX'], c='b')\n",
    "plt.plot(rets['SPY'], pred_1, 'b')\n",
    "\n",
    "plt.scatter(rets['SPY'], rets['AAPL'], c='r')\n",
    "plt.plot(rets['SPY'], pred_2, 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 3. Plot Stochastic Oscillator:\n",
    "\n",
    "$K = (\\frac{C - L14}{H14 - L14})$\n",
    "\n",
    "C - the most recent closing price\n",
    "\n",
    "L14 - the lowest price traded of the 14 previous trading days\n",
    "\n",
    "H14 - the highest price traded of the 14 previous trading days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "yf.download('SPY', start = '2019-01-01', end = '2019-12-31', period = '1d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "\n",
    "SPY = yf.download('SPY', start = '2019-01-01', end = '2019-12-31', period = '1d')\n",
    "\n",
    "window = 14\n",
    "SPY['L14'] = SPY['Low'].rolling(window=window).min()\n",
    "SPY['H14'] = SPY['High'].rolling(window=window).max()\n",
    "\n",
    "SPY['Oscillator'] = (SPY['Close'] - SPY['L14']) / (SPY['H14'] - SPY['L14'])\n",
    "ax = SPY['Oscillator'].plot(figsize=(10, 6),\n",
    "                            title='Stochastic Oscillator')\n",
    "\n",
    "ax.axhline(0.8,\n",
    "           c='r',\n",
    "           ls='--')\n",
    "\n",
    "ax.axhline(0.2,\n",
    "           c='r',\n",
    "           ls='--')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
